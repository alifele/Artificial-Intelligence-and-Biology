{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.w = self.Compile(input_shape, output_shape)\n",
    "        self.alpha=0.05\n",
    "    \n",
    "    \n",
    "    def fit_heb(self, X, Y):\n",
    "        for elem in zip(X,Y):\n",
    "            self.update_w(elem[0], elem[1], self.w)\n",
    "            \n",
    "    \n",
    "    def remove(self,x,y):\n",
    "        self.update_w(-x, y, self.w)\n",
    "    \n",
    "    \n",
    "    def add(self, x,y):\n",
    "        self.update_w(x, y, self.w)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            result.append(self.activate(self.WxX(self.w, x)))\n",
    "        \n",
    "        result = np.array(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_delta_rule_trained(self,X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            result.append(self.predict_sigmoid(x)[1])\n",
    "            \n",
    "        resutl = np.array(result)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def predict_sigmoid(self,x):\n",
    "        raw = self.WxX(self.w, x)\n",
    "        activated = self.sigmoid_bipolar(raw)\n",
    "        return raw, activated\n",
    "    \n",
    "    def fit_delta_rule(self, X,Y, epochs=10):\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for x,y in zip(X,Y):\n",
    "                raw_output, activated_output = self.predict_sigmoid(x)\n",
    "                vec1 = self.alpha*(activated_output - y)* 2*self.dev_sigmoid(raw_output)\n",
    "                vec2 = x\n",
    "                self.w += vec1.reshape((-1,1)) @ vec2.reshape((1,-1))\n",
    "\n",
    "    #### Low Level Functions ####\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_bipolar(self,x):\n",
    "        return 2*self.sigmoid(x)- 1\n",
    "    \n",
    "    def dev_sigmoid(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def Compile(self, input_shape, output_shape):\n",
    "        '''\n",
    "        this function will initialize the w_list matrix\n",
    "        the initial values will be zero\n",
    "        '''\n",
    "        w = np.zeros((output_shape, input_shape))\n",
    "        return w\n",
    "    \n",
    "    def update_w(self,x,y,w):\n",
    "        w += y.reshape((-1,1))@x.reshape((1,-1))\n",
    "    \n",
    "    \n",
    "    def activate(self,x):\n",
    "        x[x<0] = -1\n",
    "        x[x>0] = 1\n",
    "        x[x==0] = 0\n",
    "        return x\n",
    "        \n",
    "    def WxX(self,w,x):\n",
    "        result =  w @ x.reshape((-1,1))\n",
    "        return result.reshape(-1)  # returns shape less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 4\n",
    "output_shape = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,1,1,1],[-1,1,-1,-1],[1,1,1,-1],[1,-1,-1,1]]\n",
    "y = [[1],[1],[-1],[-1]]\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Hebbian rule\n",
    "\n",
    "1. to add a s:t pair to the network, update the weights matrix with the following relations:\n",
    "\n",
    "$W = W + Y.X^T$\n",
    "\n",
    "2. to remove a s:t pair from the network, update the weights matirx with following realtion:\n",
    "\n",
    "$W = W + Y^c.X^T$\n",
    "\n",
    "in which $Y^c$ can be obtained by turning all of the 1's in the $Y$ to -1 and viceversa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_heb(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.,  2.,  0.,  0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Delta Rule\n",
    "\n",
    "### 1. Activation function\n",
    "\n",
    "Since using the delta rule to trian a network requires an activation function that has derivative, so we set the activation function to be sigmoid. But note that the sigmoid function should be changed to a biplora format (with outputs that varies form -1 to 1 rather than 0 to 1 in the regular one)\n",
    "\n",
    "$\\sigma_{binary}(x) = \\dfrac{1}{1 + e^{-x}} \\Rightarrow \\sigma_{bipolar}(x) = 2 \\sigma_{binary}(x)-1 $\n",
    "\n",
    "and also for the derivative we can write:\n",
    "\n",
    "$\\sigma'_{binary}(x) = \\sigma(x)(1-\\sigma(x)) \\Rightarrow \\sigma'_{bipolar}(x) = 2\\sigma'_{binary}(x) = 2\\sigma(x)(1-\\sigma(x)) $\n",
    "\n",
    "\n",
    "### 2. updating rule\n",
    "\n",
    "$\\Delta w_{ji} = \\alpha (t_j - y_j) \\sigma_{bipolar}'(h_j) x_i = 2\\alpha (t_j - y_j) \\sigma'(h_j) x_i $\n",
    "\n",
    "in which $w_{ji}$ is the connection from neuron $i$ in the input layer to the neuron $j$ in the output layer. \n",
    "\n",
    "And also the $h_j$ is the input of neuron $j$ in which is in fact the weighted average of the output of the neurons in the previous layer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_delta_rule(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.15807176]),\n",
       " array([-0.84384648]),\n",
       " array([-0.03171385]),\n",
       " array([0.85788382])]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_delta_rule_trained(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer to comparison between methods:\n",
    "As you can see the network which is trained using the hebb learning rule is more persistent against noise and also has very lower prediction error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
